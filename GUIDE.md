# ê°•í™”í•™ìŠµ ì‹¤ìŠµ: Lunar Lander í•™ìŠµì‹œí‚¤ê¸°

## ğŸ¯ ì‹¤ìŠµ ëª©í‘œ

ì´ ì‹¤ìŠµì—ì„œëŠ” **ê°•í™”í•™ìŠµ(Reinforcement Learning)**ì˜ ëŒ€í‘œì ì¸ ë‘ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ë‹¬ ì°©ë¥™ì„ (Lunar Lander) ê²Œì„ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

- **DQN (Deep Q-Network)**: ê°€ì¹˜ ê¸°ë°˜ í•™ìŠµ
- **Actor-Critic**: ì •ì±… ê¸°ë°˜ + ê°€ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
lunar-lander/
â”œâ”€â”€ agent_class.py          # â­ ì—¬ëŸ¬ë¶„ì´ ì½”ë“œë¥¼ ì‘ì„±í•  íŒŒì¼
â”œâ”€â”€ train_agent.py          # í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_agent.py            # í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ GUIDE.md                # ì´ íŒŒì¼
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install gymnasium torch h5py numpy
```

### 2. í”„ë¡œì íŠ¸ ì´í•´í•˜ê¸°

#### Lunar Lander í™˜ê²½
- **ëª©í‘œ**: ë‹¬ ì°©ë¥™ì„ ì„ ì•ˆì „í•˜ê²Œ ì°©ë¥™ì‹œí‚¤ê¸°
- **ìƒíƒœ(State)**: 8ì°¨ì› ë²¡í„°
  - ìœ„ì¹˜ (x, y)
  - ì†ë„ (vx, vy)
  - ê°ë„, ê°ì†ë„
  - ì™¼ìª½/ì˜¤ë¥¸ìª½ ë‹¤ë¦¬ ì°©ì§€ ì—¬ë¶€
- **í–‰ë™(Action)**: 4ê°€ì§€
  - 0: ì•„ë¬´ê²ƒë„ ì•ˆ í•¨
  - 1: ì™¼ìª½ ì—”ì§„ ì í™”
  - 2: ë©”ì¸ ì—”ì§„ ì í™” (ìœ„ë¡œ)
  - 3: ì˜¤ë¥¸ìª½ ì—”ì§„ ì í™”
- **ë³´ìƒ(Reward)**:
  - ì°©ë¥™ ì„±ê³µ: +100~200ì 
  - ì¶”ë½: -100ì 
  - ì—°ë£Œ ì‚¬ìš©: ê°ì 
  - ëª©í‘œ ì§€ì ì— ê°€ê¹Œìš¸ìˆ˜ë¡: ë³´ë„ˆìŠ¤

#### ê°•í™”í•™ìŠµ í•µì‹¬ ê°œë…

**Bellman ë°©ì •ì‹**:
```
Q(s, a) = r + Î³ * max Q(s', a')
```
- `s`: í˜„ì¬ ìƒíƒœ
- `a`: í˜„ì¬ í–‰ë™
- `r`: ë°›ì€ ë³´ìƒ
- `Î³`: í• ì¸ìœ¨ (discount factor, 0.99)
- `s'`: ë‹¤ìŒ ìƒíƒœ
- `a'`: ë‹¤ìŒ í–‰ë™

## âœï¸ ì‹¤ìŠµ ê³¼ì œ

`agent_class.py` íŒŒì¼ì—ì„œ `TODO` ì£¼ì„ì´ ìˆëŠ” 4ê°œì˜ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

---

### ğŸ“ Part 1: DQN (Deep Q-Network)

#### ê³¼ì œ 1-1: `dqn.act()` - Epsilon-Greedy í–‰ë™ ì„ íƒ

**ìœ„ì¹˜**: `agent_class.py` ì•½ 781ë²ˆì§¸ ì¤„

**ëª©í‘œ**: Epsilon-greedy ì „ëµìœ¼ë¡œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

**ê°œë…**:
- **íƒí—˜(Exploration)**: í™•ë¥  Îµë¡œ ëœë¤ í–‰ë™ ì„ íƒ â†’ ìƒˆë¡œìš´ ì „ëµ ë°œê²¬
- **í™œìš©(Exploitation)**: í™•ë¥  (1-Îµ)ë¡œ ìµœì„ ì˜ í–‰ë™ ì„ íƒ â†’ í•™ìŠµëœ ì§€ì‹ í™œìš©

**êµ¬í˜„ ê°€ì´ë“œ**:
```python
def act(self, state, epsilon=0.):
    if self.in_training:
        epsilon = self.epsilon

    # TODO: ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±
    # 1. torch.rand(1).item()ìœ¼ë¡œ 0~1 ì‚¬ì´ ë‚œìˆ˜ ìƒì„±
    # 2. ë‚œìˆ˜ > epsilonì´ë©´:
    #    - policy_netìœ¼ë¡œ Qê°’ ê³„ì‚°
    #    - ê°€ì¥ ë†’ì€ Qê°’ì˜ í–‰ë™ ì„ íƒ (argmax)
    # 3. ë‚œìˆ˜ â‰¤ epsilonì´ë©´:
    #    - ëœë¤ í–‰ë™ ì„ íƒ (torch.randint ì‚¬ìš©)
```

**íŒíŠ¸**:
- `self.neural_networks['policy_net']`ë¡œ ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼
- `torch.no_grad()` ë¸”ë¡ ì•ˆì—ì„œ ì¶”ë¡ 
- `policy_net.eval()` ëª¨ë“œë¡œ ì „í™˜
- `torch.tensor(state)`ë¡œ ì…ë ¥ ë³€í™˜
- `.argmax(0).item()`ìœ¼ë¡œ í–‰ë™ ì¸ë±ìŠ¤ ì¶”ì¶œ

**í…ŒìŠ¤íŠ¸ ë°©ë²•**:
```python
# agent_class.pyì—ì„œ NotImplementedErrorë¥¼ ì œê±°í•˜ê³  êµ¬í˜„ í›„
import agent_class as agent
params = {'N_state': 8, 'N_actions': 4}
my_agent = agent.dqn(params)
# ëœë¤ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸
import numpy as np
state = np.random.randn(8)
action = my_agent.act(state, epsilon=0.1)
print(f"ì„ íƒëœ í–‰ë™: {action}")  # 0~3 ì‚¬ì´ ì •ìˆ˜ê°€ ë‚˜ì™€ì•¼ í•¨
```

---

#### ê³¼ì œ 1-2: `dqn.run_optimization_step()` - Bellman ë°©ì •ì‹ êµ¬í˜„

**ìœ„ì¹˜**: `agent_class.py` ì•½ 829ë²ˆì§¸ ì¤„

**ëª©í‘œ**: DQNì˜ í•µì‹¬ì¸ Bellman ë°©ì •ì‹ì„ ì‚¬ìš©í•œ Q-í•¨ìˆ˜ í•™ìŠµì„ êµ¬í˜„í•˜ì„¸ìš”.

**ê°œë…**:
- **Policy Network**: í˜„ì¬ í•™ìŠµ ì¤‘ì¸ Q-í•¨ìˆ˜
- **Target Network**: ì•ˆì •ì ì¸ íƒ€ê²Ÿ Qê°’ì„ ìœ„í•œ ë³µì‚¬ë³¸
- **Experience Replay**: ê³¼ê±° ê²½í—˜ì„ ì €ì¥í•˜ê³  ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµ

**DQN vs Double DQN**:
- **Standard DQN**: Target networkë¡œ í–‰ë™ ì„ íƒ + í‰ê°€
  ```python
  Q_next = target_net(s').max()
  ```
- **Double DQN**: Policy netìœ¼ë¡œ í–‰ë™ ì„ íƒ, Target netìœ¼ë¡œ í‰ê°€ (ê³¼ëŒ€í‰ê°€ ë°©ì§€)
  ```python
  a_best = policy_net(s').argmax()
  Q_next = target_net(s')[a_best]
  ```

**êµ¬í˜„ ê°€ì´ë“œ**:
```python
def run_optimization_step(self, epoch):
    # ë©”ëª¨ë¦¬ì—ì„œ ë°°ì¹˜ ìƒ˜í”Œë§ (ì´ë¯¸ êµ¬í˜„ë¨)
    state_batch, action_batch, next_state_batch, reward_batch, done_batch = ...

    # TODO: ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±

    # [Step 1] LHS ê³„ì‚° (í˜„ì¬ Qê°’)
    # LHS = policy_net(state_batch).gather(dim=1, index=action_batch.unsqueeze(1))

    # [Step 2] RHS ê³„ì‚° (íƒ€ê²Ÿ Qê°’)
    # if self.doubleDQN:
    #     # Double DQN: policy netìœ¼ë¡œ í–‰ë™ ì„ íƒ
    #     argmax_next_state = policy_net(next_state_batch).argmax(dim=1)
    #     # target netìœ¼ë¡œ Qê°’ í‰ê°€
    #     Q_next_state = target_net(next_state_batch).gather(...)
    # else:
    #     # Standard DQN: target netìœ¼ë¡œ ì§ì ‘ ìµœëŒ“ê°’
    #     Q_next_state = target_net(next_state_batch).max(1)[0].detach()
    #
    # RHS = Q_next_state * self.discount_factor * (1 - done_batch) + reward_batch
    # RHS = RHS.unsqueeze(1)  # shape ë§ì¶”ê¸°

    # [Step 3] ì†ì‹¤ ê³„ì‚° ë° ìµœì í™”
    # loss_ = loss(LHS, RHS)
    # optimizer.zero_grad()
    # loss_.backward()
    # optimizer.step()
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- `done_batch`: ì—í”¼ì†Œë“œê°€ ëë‚¬ìœ¼ë©´ ë‹¤ìŒ ìƒíƒœì˜ Qê°’ì€ 0
- `.detach()`: Target networkì˜ gradient ê³„ì‚° ë°©ì§€
- `.gather()`: íŠ¹ì • ì¸ë±ìŠ¤ì˜ ê°’ë§Œ ì„ íƒ

---

### ğŸ“ Part 2: Actor-Critic

#### ê³¼ì œ 2-1: `actor_critic.act()` - í™•ë¥ ì  ì •ì±…

**ìœ„ì¹˜**: `agent_class.py` ì•½ 983ë²ˆì§¸ ì¤„

**ëª©í‘œ**: Actor ë„¤íŠ¸ì›Œí¬ê°€ ì¶œë ¥í•œ í™•ë¥  ë¶„í¬ì—ì„œ í–‰ë™ì„ ìƒ˜í”Œë§í•˜ì„¸ìš”.

**ê°œë…**:
- **DQNê³¼ì˜ ì°¨ì´ì **:
  - DQN: ê²°ì •ë¡ ì  (í•­ìƒ Qê°’ì´ ìµœëŒ€ì¸ í–‰ë™ ì„ íƒ)
  - Actor-Critic: í™•ë¥ ì  (í™•ë¥  ë¶„í¬ì—ì„œ ìƒ˜í”Œë§)
- **ì¥ì **: ë” ë¶€ë“œëŸ¬ìš´ íƒí—˜, ì—°ì†ì ì¸ ì •ì±… ê°œì„ 

**êµ¬í˜„ ê°€ì´ë“œ**:
```python
def act(self, state):
    actor_net = self.neural_networks['policy_net']

    # TODO: ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±
    # with torch.no_grad():
    #     actor_net.eval()
    #     # 1. actor_netì— state ì…ë ¥ â†’ affinities(ì„ í˜¸ë„) ì¶œë ¥
    #     # 2. Softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
    #     probs = self.Softmax(actor_net(torch.tensor(state)))
    #     # 3. Categorical ë¶„í¬ ìƒì„±
    #     m = Categorical(probs)
    #     # 4. ìƒ˜í”Œë§
    #     action = m.sample()
    #     actor_net.train()
    #     return action.item()
```

**ì˜ˆì‹œ**:
```
State: [x=0.5, y=1.2, vx=-0.3, ...]
Actor Net ì¶œë ¥: [2.1, -0.5, 1.3, 0.8]  (affinities)
Softmax í›„:     [0.55, 0.04, 0.26, 0.15]  (í™•ë¥ )
â†’ 55% í™•ë¥ ë¡œ í–‰ë™ 0, 26% í™•ë¥ ë¡œ í–‰ë™ 2 ì„ íƒ
```

---

#### ê³¼ì œ 2-2: `actor_critic.run_optimization_step()` - Actor-Critic í•™ìŠµ

**ìœ„ì¹˜**: `agent_class.py` ì•½ 1020ë²ˆì§¸ ì¤„

**ëª©í‘œ**: Actorì™€ Critic ë‘ ë„¤íŠ¸ì›Œí¬ë¥¼ ë™ì‹œì— í•™ìŠµì‹œí‚¤ì„¸ìš”.

**ê°œë…**:
- **Critic (ê°€ì¹˜ í•¨ìˆ˜)**: ìƒíƒœì˜ ì¢‹ê³  ë‚˜ì¨ì„ í‰ê°€ â†’ V(s)
- **Actor (ì •ì±…)**: ì–´ë–¤ í–‰ë™ì„ í• ì§€ ê²°ì • â†’ Ï€(a|s)
- **Advantage**: A = V(s') - V(s) â†’ "ì´ í–‰ë™ì´ í‰ê· ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€?"

**êµ¬í˜„ ê°€ì´ë“œ**:
```python
def run_optimization_step(self, epoch):
    # ë°°ì¹˜ ìƒ˜í”Œë§ (ì´ë¯¸ êµ¬í˜„ë¨)
    state_batch, action_batch, next_state_batch, reward_batch, done_batch = ...

    # TODO: ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±

    # ========== Part 1: Critic í•™ìŠµ ==========
    # critic_net.train()
    #
    # LHS = critic_net(state_batch)  # V(s)
    # Q_next_state = critic_net(next_state_batch).detach().squeeze(1)
    # RHS = Q_next_state * self.discount_factor * (1 - done_batch) + reward_batch
    # RHS = RHS.unsqueeze(1)
    #
    # loss = loss_critic(LHS, RHS)
    # optimizer_critic.zero_grad()
    # loss.backward()
    # optimizer_critic.step()
    #
    # critic_net.eval()

    # ========== Part 2: Actor í•™ìŠµ ==========
    # actor_net.train()
    #
    # advantage_batch = (RHS - LHS).detach()  # Advantage ê³„ì‚°
    #
    # loss = loss_actor(state_batch, action_batch, advantage_batch)
    # optimizer_actor.zero_grad()
    # loss.backward()
    # optimizer_actor.step()
    #
    # actor_net.eval()
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- Criticì„ ë¨¼ì € í•™ìŠµ â†’ ì •í™•í•œ Advantage ê³„ì‚°
- Advantageê°€ ì–‘ìˆ˜: í‰ê· ë³´ë‹¤ ì¢‹ì€ í–‰ë™ â†’ í•´ë‹¹ í–‰ë™ í™•ë¥  ì¦ê°€
- Advantageê°€ ìŒìˆ˜: í‰ê· ë³´ë‹¤ ë‚˜ìœ í–‰ë™ â†’ í•´ë‹¹ í–‰ë™ í™•ë¥  ê°ì†Œ

---

## ğŸƒ ì‹¤í–‰í•˜ê¸°

### 1. DQN í•™ìŠµ

```bash
# Standard DQN
python train_agent.py --f my_dqn --dqn --verbose

# Double DQN (ì¶”ì²œ!)
python train_agent.py --f my_ddqn --ddqn --verbose
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
| episode | return          | minimal return      | mean return        |
|         | (this episode)  | (last 20 episodes)  | (last 20 episodes) |
|-------------------------------------------------------------------------|
|       0 |     -245.123    |       -245.123      |      -245.123      |
|       1 |     -189.456    |       -245.123      |      -217.290      |
...
|     523 |      234.567    |        201.234      |       231.890      |  â† ì„±ê³µ!
```

### 2. Actor-Critic í•™ìŠµ

```bash
python train_agent.py --f my_ac --verbose
```

### 3. í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€

```bash
# DQN í‰ê°€ (100 ì—í”¼ì†Œë“œ)
python run_agent.py --f my_dqn --dqn --N 100 --verbose

# Actor-Critic í‰ê°€
python run_agent.py --f my_ac --N 100 --verbose
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
Run 1 of 100 completed with return 245.3. Mean return over all episodes so far = 245.3
Run 2 of 100 completed with return 189.7. Mean return over all episodes so far = 217.5
...
Run 100 of 100 completed with return 251.2. Mean return over all episodes so far = 227.8
```

---

## ğŸ› ë¬¸ì œ í•´ê²° (Troubleshooting)

### âŒ NotImplementedError ë°œìƒ
```python
NotImplementedError: TODO: DQN act í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”
```
â†’ `agent_class.py`ì—ì„œ í•´ë‹¹ í•¨ìˆ˜ì˜ `raise NotImplementedError(...)`ë¥¼ ì§€ìš°ê³  ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

### âŒ í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
- DQNì€ ë³´í†µ 300~800 ì—í”¼ì†Œë“œ ì†Œìš” (10~30ë¶„)
- Actor-Criticì€ ì•½ 30% ë” ì˜¤ë˜ ê±¸ë¦¼
- `--verbose` í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í™•ì¸

### âŒ í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ
1. **Epsilon ë¬¸ì œ** (DQN):
   - ë„ˆë¬´ ë¹¨ë¦¬ ê°ì†Œ: `d_epsilon`ì„ ì¤„ì´ê¸° (ì˜ˆ: 0.00005 â†’ 0.00003)
   - ë„ˆë¬´ ëŠë¦¬ê²Œ ê°ì†Œ: `d_epsilon`ì„ ëŠ˜ë¦¬ê¸°

2. **Learning Rate ë¬¸ì œ**:
   ```python
   # train_agent.pyì—ì„œ ìˆ˜ì •
   'optimizer_args': {'lr': 1e-3}  # 1e-4ë¡œ ì¤„ì´ê±°ë‚˜ 1e-2ë¡œ ëŠ˜ë¦¬ê¸°
   ```

3. **Batch Size ë¬¸ì œ**:
   ```python
   'batch_size': 32  # 16 ë˜ëŠ” 64ë¡œ ë³€ê²½
   ```

### âŒ Shape ì—ëŸ¬
```python
RuntimeError: The size of tensor a (32) must match the size of tensor b (1)
```
â†’ `.unsqueeze(1)` ë˜ëŠ” `.squeeze(1)`ë¡œ ì°¨ì› ì¡°ì • í•„ìš”

---

## ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„

### ìƒì„±ë˜ëŠ” íŒŒì¼
- `my_dqn.tar`: í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜
- `my_dqn_training_data.h5`: í•™ìŠµ í†µê³„
  - ì—í”¼ì†Œë“œë³„ return
  - ì—í”¼ì†Œë“œë³„ duration
  - ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
- `my_dqn_execution_time.txt`: í•™ìŠµ ì†Œìš” ì‹œê°„

### ì¢‹ì€ í•™ìŠµì˜ ì§€í‘œ
âœ… í‰ê·  returnì´ 200 ì´ìƒ
âœ… ì—í”¼ì†Œë“œê°€ ì§§ì•„ì§ (ë¹ ë¥¸ ì°©ë¥™)
âœ… ì•ˆì •ì ì¸ return (ë¶„ì‚°ì´ ì‘ìŒ)

---

## ğŸ“ ì‹¬í™” í•™ìŠµ

### 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
`train_agent.py`ì˜ `parameters` ë”•ì…”ë„ˆë¦¬ë¥¼ ìˆ˜ì •í•˜ì—¬ ë‹¤ì–‘í•œ ì„¤ì • ì‹¤í—˜:
- Discount factor (0.95 ~ 0.99)
- Learning rate (1e-4 ~ 1e-2)
- Network architecture (hidden layers)

### 2. ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- ê°™ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ DQN vs Actor-Critic ë¹„êµ
- ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì´ ë” ë¹ ë¥´ê²Œ í•™ìŠµí•˜ë‚˜?
- ìµœì¢… ì„±ëŠ¥ ì°¨ì´ëŠ”?

### 3. ì‹œê°í™”
Jupyter Notebookì„ ì‚¬ìš©í•˜ì—¬:
- í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„
- Return ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
- ì—í”¼ì†Œë“œ ê¸¸ì´ ë³€í™”

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
1. **DQN**: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) (Mnih et al., 2013)
2. **Double DQN**: [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (van Hasselt et al., 2015)
3. **Actor-Critic**: [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html) (Sutton & Barto)

### ê°œë… ì •ë¦¬
- **Experience Replay**: ê³¼ê±° ê²½í—˜ì„ ì €ì¥í•˜ê³  ì¬ì‚¬ìš© â†’ í•™ìŠµ ì•ˆì •í™”
- **Target Network**: ëŠë¦¬ê²Œ ì—…ë°ì´íŠ¸ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ â†’ íƒ€ê²Ÿ ê°’ ì•ˆì •í™”
- **Epsilon-Greedy**: Exploration vs Exploitation ê· í˜•
- **Advantage**: í–‰ë™ì˜ ìƒëŒ€ì  ê°€ì¹˜ í‰ê°€

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤ìŠµì„ ì™„ë£Œí–ˆë‹¤ë©´ ì²´í¬í•´ë³´ì„¸ìš”:

- [ ] `dqn.act()` í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ
- [ ] `dqn.run_optimization_step()` í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ
- [ ] DQNìœ¼ë¡œ ì—ì´ì „íŠ¸ í•™ìŠµ ì„±ê³µ (í‰ê·  return > 200)
- [ ] `actor_critic.act()` í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ
- [ ] `actor_critic.run_optimization_step()` í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ
- [ ] Actor-Criticìœ¼ë¡œ ì—ì´ì „íŠ¸ í•™ìŠµ ì„±ê³µ
- [ ] ë‘ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ë¶„ì„

---

## ğŸ’¬ ì§ˆë¬¸ì´ ìˆë‚˜ìš”?

êµ¬í˜„ ì¤‘ ë§‰íˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´:
1. TODO ì£¼ì„ì˜ íŒíŠ¸ë¥¼ ë‹¤ì‹œ ì½ì–´ë³´ì„¸ìš”
2. PyTorch ê³µì‹ ë¬¸ì„œ ì°¸ê³ : https://pytorch.org/docs/stable/index.html
3. Gymnasium í™˜ê²½ ë¬¸ì„œ: https://gymnasium.farama.org/environments/box2d/lunar_lander/

**Happy Learning! ğŸš€**
